version: '3.8'

services:
  docling-inference:
    build: .
    image: docling-inference:latest
    ports:
      - 8877:8080
    environment:
      - DEV_MODE=0
      - AUTH_TOKEN=dev-key
      - NUM_WORKERS=12
    volumes:
      - ./logs:/app/logs
      - hf_cache:/root/.cache/huggingface
      - ocr_cache:/root/.EasyOCR
    # Standard Docker configuration for GPU
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [compute, utility]

volumes:
  hf_cache:
  ocr_cache: 

  services:
  docling-inference:
    # ... other settings ...
    deploy:  # Remove this entire block
    device_requests:
      - count: all
        capabilities: ["gpu"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Explicitly expose all GPUs
      - CUDA_VISIBLE_DEVICES=0       # Use specific GPU if needed
